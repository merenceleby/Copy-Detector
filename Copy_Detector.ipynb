{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YujaAqNFs5iQ",
        "outputId": "7aa27ab3-fa54-4f23-80fc-3a99f80a4231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def load_documents_from_files(file_paths):\n",
        "    documents = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                document_text = file.read()\n",
        "                words = word_tokenize(document_text.lower())\n",
        "                words = [lemmatizer.lemmatize(word) for word in words]\n",
        "                words = [word for word in words if word not in stop_words]\n",
        "                cleaned_text = ' '.join(words)\n",
        "                documents.append(cleaned_text)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"{file_path} file not found.\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "assignments = [os.path.join(\"assignments\", file) for file in os.listdir(\"assignments\")\n",
        "                    if os.path.isfile(os.path.join(\"assignments\", file)) and file.endswith(\".txt\")]\n",
        "\n",
        "documents = load_documents_from_files(assignments)\n",
        "\n",
        "\n",
        "model = Doc2Vec.load(\"/content/model.bin\")\n",
        "\n",
        "document_vectors = [model.infer_vector(doc.split()) for doc in documents]\n",
        "similarity_matrix = cosine_similarity(document_vectors)\n",
        "\n",
        "\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i + 1, len(documents)):\n",
        "        similarity = similarity_matrix[i][j]\n",
        "        print(f\"Similarity between {os.path.basename(assignments[i])} and {os.path.basename(assignments[j])}: {similarity:.4f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "similarity_threshold = 0.75\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i + 1, len(documents)):\n",
        "        if similarity_matrix[i][j] > similarity_threshold:\n",
        "            print(f\"Document {os.path.basename(assignments[i])} and Document {os.path.basename(assignments[j])} have high similarity! And may be copy!.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v16LGGP8tJJ_",
        "outputId": "42f02b60-8564-4b92-8d58-34d85409137b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between as5.txt and as6.txt: 0.9918\n",
            "Similarity between as5.txt and as2.txt: 0.1670\n",
            "Similarity between as5.txt and as4.txt: 0.9892\n",
            "Similarity between as5.txt and as1.txt: 0.3427\n",
            "Similarity between as5.txt and as3.txt: 0.2707\n",
            "Similarity between as6.txt and as2.txt: 0.1799\n",
            "Similarity between as6.txt and as4.txt: 0.9892\n",
            "Similarity between as6.txt and as1.txt: 0.3610\n",
            "Similarity between as6.txt and as3.txt: 0.2711\n",
            "Similarity between as2.txt and as4.txt: 0.1818\n",
            "Similarity between as2.txt and as1.txt: 0.3910\n",
            "Similarity between as2.txt and as3.txt: 0.1865\n",
            "Similarity between as4.txt and as1.txt: 0.3523\n",
            "Similarity between as4.txt and as3.txt: 0.2931\n",
            "Similarity between as1.txt and as3.txt: 0.1231\n",
            "\n",
            "\n",
            "Document as5.txt and Document as6.txt have high similarity! And may be copy!.\n",
            "Document as5.txt and Document as4.txt have high similarity! And may be copy!.\n",
            "Document as6.txt and Document as4.txt have high similarity! And may be copy!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8Zp3MiAuGiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}